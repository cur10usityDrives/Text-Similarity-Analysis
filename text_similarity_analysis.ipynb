{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNX+IaGRqQpQzNB4FEGbdAR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cur10usityDrives/Text-Similarity-Analysis/blob/main/text_similarity_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3jVJZC4EjjrZ",
        "outputId": "855c427a-ee8d-4f03-af74-9538af2b98f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "english_words = set(nltk.corpus.words.words())\n",
        "\n",
        "# Preprocess the English words corpus\n",
        "def preprocess_corpus(corpus):\n",
        "    preprocessed_corpus = []\n",
        "    for word in corpus:\n",
        "        # Tokenize the word (treat each word as a \"document\")\n",
        "        tokens = [word]\n",
        "        # Remove stop words\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "        # Stem words\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "        preprocessed_corpus.append(tokens)\n",
        "    return preprocessed_corpus\n",
        "\n",
        "preprocessed_corpus = preprocess_corpus(english_words)\n",
        "\n",
        "def preprocess_text(text, to_stem=True, to_remove_stop_words=True):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    if to_remove_stop_words:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "    if to_stem:\n",
        "        stemmer = PorterStemmer()\n",
        "        stem_tokens = [stemmer.stem(token) for token in tokens]\n",
        "        return ' '.join(stem_tokens)\n",
        "    else:\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "def encode_sentence(sentence, encoding_type='one-hot', to_stem=True, to_remove_stop_words=True):\n",
        "    processed_sentence = preprocess_text(sentence, to_stem=to_stem, to_remove_stop_words=to_remove_stop_words)\n",
        "    if encoding_type == 'one-hot':\n",
        "        vectorizer = CountVectorizer(binary=True)\n",
        "    elif encoding_type == 'bag-of-words':\n",
        "        vectorizer = CountVectorizer()\n",
        "    elif encoding_type == 'tf':\n",
        "        vectorizer = CountVectorizer()\n",
        "    elif encoding_type == 'tfidf':\n",
        "        vectorizer = TfidfVectorizer()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported encoding type\")\n",
        "    encoded_sentence = vectorizer.fit_transform([processed_sentence]).toarray()\n",
        "    return encoded_sentence.flatten()\n",
        "\n",
        "def encode_and_compare_sentences(sentence1, sentence2, encoding_type='one-hot', to_stem=True, to_remove_stop_words=True):\n",
        "    encoded_sentence1 = encode_sentence(sentence1, encoding_type=encoding_type, to_stem=to_stem, to_remove_stop_words=to_remove_stop_words)\n",
        "    encoded_sentence2 = encode_sentence(sentence2, encoding_type=encoding_type, to_stem=to_stem, to_remove_stop_words=to_remove_stop_words)\n",
        "\n",
        "    # Pad the shorter sentence with zeros to match the length of the longer one\n",
        "    max_length = max(len(encoded_sentence1), len(encoded_sentence2))\n",
        "    encoded_sentence1 = np.pad(encoded_sentence1, (0, max_length - len(encoded_sentence1)))\n",
        "    encoded_sentence2 = np.pad(encoded_sentence2, (0, max_length - len(encoded_sentence2)))\n",
        "\n",
        "    similarity = np.dot(encoded_sentence1, encoded_sentence2) / (np.linalg.norm(encoded_sentence1) * np.linalg.norm(encoded_sentence2))\n",
        "    return similarity\n",
        "\n",
        "def evaluate_scenarios(scenarios):\n",
        "    for scenario in scenarios:\n",
        "        print(f\"Scenario: {scenario['name']}\")\n",
        "        pairs = scenario.get('pairs', [])\n",
        "        for i, pair in enumerate(pairs, 1):\n",
        "            sentence1 = pair['sentence1']\n",
        "            sentence2 = pair['sentence2']\n",
        "            to_stem = scenario.get('to_stem', True)\n",
        "            to_remove_stop_words = scenario.get('to_remove_stop_words', True)\n",
        "            encoding_type = scenario['encoding_type']\n",
        "\n",
        "            # Original form\n",
        "            similarity_original = encode_and_compare_sentences(sentence1, sentence2, encoding_type=encoding_type, to_stem=False, to_remove_stop_words=False)\n",
        "\n",
        "            # After stop-word removal/stemming\n",
        "            sentence1_processed = preprocess_text(sentence1, to_stem=to_stem, to_remove_stop_words=to_remove_stop_words)\n",
        "            sentence2_processed = preprocess_text(sentence2, to_stem=to_stem, to_remove_stop_words=to_remove_stop_words)\n",
        "            similarity_processed = encode_and_compare_sentences(sentence1, sentence2, encoding_type=encoding_type, to_stem=to_stem, to_remove_stop_words=to_remove_stop_words)\n",
        "\n",
        "            print(f\"Pair {i}:\")\n",
        "            print(f\"Original Similarity: {similarity_original if similarity_original < 1 else round(similarity_original, 2)}\")\n",
        "            print(f\"After Stop-word removal/Stemming Similarity: {similarity_processed if similarity_processed < 1 else round(similarity_processed, 2)}\")\n",
        "            print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test scenarios\n",
        "scenarios = [\n",
        "    {\n",
        "        'name': 'Stop-word removal only with ohe',\n",
        "        'to_stem': False,\n",
        "        'to_remove_stop_words': True,\n",
        "        'encoding_type': 'one-hot',\n",
        "        'pairs': [\n",
        "            {\n",
        "                'sentence1': \"The cat chased the mouse around the house.\",\n",
        "                'sentence2': \"A mouse was not being chased by a cat inside the house.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"The brown dog is lazy.\",\n",
        "                'sentence2': \"This is not a lazy brown dog.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"The quick brown fox jumps over the lazy dog.\",\n",
        "                'sentence2': \"A fast brown fox leaps over a tired dog.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"This is a very excruciating task targeting the dedicated route.\",\n",
        "                'sentence2': \"Excruciating tasks ought to be targeted with dedication.\"\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        'name': 'Stemming only with ohe',\n",
        "        'to_stem': True,\n",
        "        'to_remove_stop_words': False,\n",
        "        'encoding_type': 'one-hot',\n",
        "        'pairs': [\n",
        "            {\n",
        "                'sentence1': \"The cat chased the mouse around the house.\",\n",
        "                'sentence2': \"A mouse was not being chased by a cat inside the house.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"The brown dog is lazy.\",\n",
        "                'sentence2': \"This is not a lazy brown dog.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"The quick brown fox jumps over the lazy dog.\",\n",
        "                'sentence2': \"A fast brown fox leaps over a tired dog.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"This is a very excruciating task targeting the dedicated route.\",\n",
        "                'sentence2': \"Excruciating tasks ought to be targeted with dedication.\"\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        'name': 'Stop-word removal and Stemming with ohe',\n",
        "        'to_stem': True,\n",
        "        'to_remove_stop_words': True,\n",
        "        'encoding_type': 'one-hot',\n",
        "        'pairs': [\n",
        "            {\n",
        "                'sentence1': \"The cat chased the mouse around the house.\",\n",
        "                'sentence2': \"A mouse was not being chased by a cat inside the house.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"The brown dog is lazy.\",\n",
        "                'sentence2': \"This is not a lazy brown dog.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"The quick brown fox jumps over the lazy dog.\",\n",
        "                'sentence2': \"A fast brown fox leaps over a tired dog.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"This is a very excruciating task targeting the dedicated route.\",\n",
        "                'sentence2': \"Excruciating tasks ought to be targeted with dedication.\"\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        'name': 'Stop-word removal and Stemming with tfxidf',\n",
        "        'to_stem': True,\n",
        "        'to_remove_stop_words': True,\n",
        "        'encoding_type': 'tfidf',\n",
        "        'pairs': [\n",
        "            {\n",
        "                'sentence1': \"The cat chased the mouse around the house.\",\n",
        "                'sentence2': \"A mouse was not being chased by a cat inside the house.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"The brown dog is lazy.\",\n",
        "                'sentence2': \"This is not a lazy brown dog.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"The quick brown fox jumps over the lazy dog.\",\n",
        "                'sentence2': \"A fast brown fox leaps over a tired dog.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"This is a very excruciating task targeting the dedicated route.\",\n",
        "                'sentence2': \"Excruciating tasks ought to be targeted with dedication.\"\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "        {\n",
        "        'name': 'Stop-word removal and Stemming with tf',\n",
        "        'to_stem': True,\n",
        "        'to_remove_stop_words': True,\n",
        "        'encoding_type': 'tf',\n",
        "        'pairs': [\n",
        "            {\n",
        "                'sentence1': \"The cat chased the mouse around the house.\",\n",
        "                'sentence2': \"A mouse was not being chased by a cat inside the house.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"The brown dog is lazy.\",\n",
        "                'sentence2': \"This is not a lazy brown dog.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"The quick brown fox jumps over the lazy dog.\",\n",
        "                'sentence2': \"A fast brown fox leaps over a tired dog.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"This is a very excruciating task targeting the dedicated route.\",\n",
        "                'sentence2': \"Excruciating tasks ought to be targeted with dedication.\"\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        'name': 'Stop-word removal and Stemming with bag-of-words',\n",
        "        'to_stem': True,\n",
        "        'to_remove_stop_words': True,\n",
        "        'encoding_type': 'bag-of-words',\n",
        "        'pairs': [\n",
        "            {\n",
        "                'sentence1': \"The cat chased the mouse around the house.\",\n",
        "                'sentence2': \"A mouse was not being chased by a cat inside the house.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"The brown dog is lazy.\",\n",
        "                'sentence2': \"This is not a lazy brown dog.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"The quick brown fox jumps over the lazy dog.\",\n",
        "                'sentence2': \"A fast brown fox leaps over a tired dog.\"\n",
        "            },\n",
        "            {\n",
        "                'sentence1': \"This is a very excruciating task targeting the dedicated route.\",\n",
        "                'sentence2': \"Excruciating tasks ought to be targeted with dedication.\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    # Add more scenarios as needed\n",
        "]\n",
        "\n",
        "evaluate_scenarios(scenarios)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI24_uoVaTSp",
        "outputId": "ea8a26d6-c51b-4698-b352-365cfe5da708"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scenario: Stop-word removal only with ohe\n",
            "Pair 1:\n",
            "Original Similarity: 0.7745966692414834\n",
            "After Stop-word removal/Stemming Similarity: 0.9999999999999998\n",
            "\n",
            "Pair 2:\n",
            "Original Similarity: 0.9128709291752769\n",
            "After Stop-word removal/Stemming Similarity: 1.0\n",
            "\n",
            "Pair 3:\n",
            "Original Similarity: 0.9354143466934852\n",
            "After Stop-word removal/Stemming Similarity: 1.0\n",
            "\n",
            "Pair 4:\n",
            "Original Similarity: 0.9428090415820632\n",
            "After Stop-word removal/Stemming Similarity: 0.9999999999999998\n",
            "\n",
            "Scenario: Stemming only with ohe\n",
            "Pair 1:\n",
            "Original Similarity: 0.7745966692414834\n",
            "After Stop-word removal/Stemming Similarity: 0.7745966692414834\n",
            "\n",
            "Pair 2:\n",
            "Original Similarity: 0.9128709291752769\n",
            "After Stop-word removal/Stemming Similarity: 0.9128709291752769\n",
            "\n",
            "Pair 3:\n",
            "Original Similarity: 0.9354143466934852\n",
            "After Stop-word removal/Stemming Similarity: 0.9354143466934852\n",
            "\n",
            "Pair 4:\n",
            "Original Similarity: 0.9428090415820632\n",
            "After Stop-word removal/Stemming Similarity: 0.9428090415820632\n",
            "\n",
            "Scenario: Stop-word removal and Stemming with ohe\n",
            "Pair 1:\n",
            "Original Similarity: 0.7745966692414834\n",
            "After Stop-word removal/Stemming Similarity: 0.9999999999999998\n",
            "\n",
            "Pair 2:\n",
            "Original Similarity: 0.9128709291752769\n",
            "After Stop-word removal/Stemming Similarity: 1.0\n",
            "\n",
            "Pair 3:\n",
            "Original Similarity: 0.9354143466934852\n",
            "After Stop-word removal/Stemming Similarity: 1.0\n",
            "\n",
            "Pair 4:\n",
            "Original Similarity: 0.9428090415820632\n",
            "After Stop-word removal/Stemming Similarity: 0.9999999999999998\n",
            "\n",
            "Scenario: Stop-word removal and Stemming with tfxidf\n",
            "Pair 1:\n",
            "Original Similarity: 0.6761234037828134\n",
            "After Stop-word removal/Stemming Similarity: 1.0\n",
            "\n",
            "Pair 2:\n",
            "Original Similarity: 0.912870929175277\n",
            "After Stop-word removal/Stemming Similarity: 1.0\n",
            "\n",
            "Pair 3:\n",
            "Original Similarity: 0.7977240352174659\n",
            "After Stop-word removal/Stemming Similarity: 1.0\n",
            "\n",
            "Pair 4:\n",
            "Original Similarity: 0.9428090415820635\n",
            "After Stop-word removal/Stemming Similarity: 1.0\n",
            "\n",
            "Scenario: Stop-word removal and Stemming with tf\n",
            "Pair 1:\n",
            "Original Similarity: 0.6761234037828132\n",
            "After Stop-word removal/Stemming Similarity: 0.9999999999999998\n",
            "\n",
            "Pair 2:\n",
            "Original Similarity: 0.9128709291752769\n",
            "After Stop-word removal/Stemming Similarity: 1.0\n",
            "\n",
            "Pair 3:\n",
            "Original Similarity: 0.7977240352174656\n",
            "After Stop-word removal/Stemming Similarity: 1.0\n",
            "\n",
            "Pair 4:\n",
            "Original Similarity: 0.9428090415820632\n",
            "After Stop-word removal/Stemming Similarity: 0.9999999999999998\n",
            "\n",
            "Scenario: Stop-word removal and Stemming with bag-of-words\n",
            "Pair 1:\n",
            "Original Similarity: 0.6761234037828132\n",
            "After Stop-word removal/Stemming Similarity: 0.9999999999999998\n",
            "\n",
            "Pair 2:\n",
            "Original Similarity: 0.9128709291752769\n",
            "After Stop-word removal/Stemming Similarity: 1.0\n",
            "\n",
            "Pair 3:\n",
            "Original Similarity: 0.7977240352174656\n",
            "After Stop-word removal/Stemming Similarity: 1.0\n",
            "\n",
            "Pair 4:\n",
            "Original Similarity: 0.9428090415820632\n",
            "After Stop-word removal/Stemming Similarity: 0.9999999999999998\n",
            "\n"
          ]
        }
      ]
    }
  ]
}